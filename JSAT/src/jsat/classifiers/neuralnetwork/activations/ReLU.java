package jsat.classifiers.neuralnetwork.activations;

import jsat.linear.Matrix;
import jsat.linear.Vec;

/**
 * This Activation Layer is for <b>Re</b>ctified <b>L</b>inear <b>U</b>nits. A ReLU activation is simply f(x) = max(0,
 * x), and is thus very fast to compute.
 * <br>
 * See: Nair, V., &amp; Hinton, G. E. (2010). <i>Rectified Linear Units Improve Restricted Boltzmann Machines</i>.
 * Proceedings of the 27th International Conference on Machine Learning, 807â€“814.
 *
 * @author Edward Raff
 */
public class ReLU implements ActivationLayer {

  private static final long serialVersionUID = -6691240473485759789L;

  @Override
  public void activate(Vec input, Vec output) {
    for (int i = 0; i < input.length(); i++) {
      output.set(i, Math.max(0, input.get(i)));
    }
  }

  @Override
  public void activate(Matrix input, Matrix output, boolean rowMajor) {
    for (int i = 0; i < input.rows(); i++) {
      for (int j = 0; j < input.cols(); j++) {
        output.set(i, j, Math.max(0, input.get(i, j)));
      }
    }
  }

  @Override
  public void backprop(Vec input, Vec output, Vec delta_partial, Vec errout) {
    for (int i = 0; i < input.length(); i++) {
      double out_i = output.get(i);
      if (out_i != 0) {
        errout.set(i, delta_partial.get(i));
      } else {
        errout.set(i, 0.0);
      }
    }
  }

  @Override
  public void backprop(Matrix input, Matrix output, Matrix delta_partial, Matrix errout, boolean rowMajor) {
    for (int i = 0; i < input.rows(); i++) {
      for (int j = 0; j < input.cols(); j++) {
        if (output.get(i, j) != 0) {
          errout.set(i, j, delta_partial.get(i, j));
        } else {
          errout.set(i, j, 0.0);
        }
      }
    }
  }

  @Override
  public ReLU clone() {
    return new ReLU();
  }

}
