package jsat.classifiers.linear.kernelized;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Random;
import java.util.concurrent.ExecutorService;
import jsat.DataSet;
import jsat.classifiers.CategoricalResults;
import jsat.classifiers.ClassificationDataSet;
import jsat.classifiers.Classifier;
import jsat.classifiers.DataPoint;
import jsat.classifiers.svm.SupportVectorLearner;
import jsat.distributions.Distribution;
import jsat.distributions.LogUniform;
import jsat.distributions.kernels.KernelTrick;
import jsat.exceptions.FailedToFitException;
import jsat.linear.Vec;
import jsat.parameters.Parameter;
import jsat.parameters.Parameterized;
import jsat.utils.IntList;
import jsat.utils.ListUtils;
import jsat.utils.random.XORWOW;

/**
 * An implementation of Conservative Stochastic Kernel Logistic Regression. This is an online algorithm that obtains
 * sparse solutions by conservatively rejecting updates based on a binomial distribution of the error on each update.
 * <br><br>
 * This algorithm works best on data sets with a very high number of samples where a high accuracy is obtainable using a
 * kernel model. It is often the case that this model produces accurate results, but has a low confidence due to the
 * conservative updating. This can be counteracted by having a very large number of features, but that often increases
 * the size of the model.
 * <br>
 * This batch version can also be used to more efficiently learn dense KLR models using the stochastic method with the
 * {@link CSKLR.UpdateMode#NC} mode if model sparsity is not important.
 * <br><br>
 * It is important to read the documentation and test some different values for the
 * {@link #setEta(double) learning rate} and {@link #setGamma(double) gamma} variables. They behave different compared
 * to many algorithms.
 * <br><br>
 * See paper: <br>
 * Zhang, L., Jin, R., Chen, C., Bu, J.,&amp;He, X. (2012). <i>Efficient Online Learning for Large-Scale Sparse Kernel
 * Logistic Regression</i>. Twenty-Sixth AAAI Conference on Artificial Intelligence (pp. 1219â€“1225). Retrieved from
 * <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPDFInterstitial/5003/5544">here</a>
 *
 * @author Edward Raff
 */
public class CSKLRBatch extends SupportVectorLearner implements Parameterized, Classifier {

  private static final long serialVersionUID = -2305532659182911285L;

  private double eta;
  private double curNorm;
  private double R = 10;
  private int T = 0;
  private CSKLR.UpdateMode mode;
  protected double gamma = 2;
  private int epochs = 10;

  /**
   * Creates a new SCKLR Batch learning object
   *
   * @param eta the learning rate to use
   * @param kernel the kernel to use
   * @param R the maximal norm of the surface
   * @param mode the mode to use
   * @param cacheMode the kernel caching mode to use
   */
  public CSKLRBatch(double eta, KernelTrick kernel, double R, CSKLR.UpdateMode mode, CacheMode cacheMode) {
    super(kernel, cacheMode);
    setEta(eta);
    setR(R);
    setMode(mode);
  }

  /**
   * Copy constructor
   *
   * @param toClone the object to copy
   */
  protected CSKLRBatch(CSKLRBatch toClone) {
    super(toClone);

    this.curNorm = toClone.curNorm;
    this.epochs = toClone.epochs;
    this.eta = toClone.eta;
    this.R = toClone.R;
    this.T = toClone.T;
    this.mode = toClone.mode;
    this.gamma = toClone.gamma;

  }

  @Override
  public CSKLRBatch clone() {
    return new CSKLRBatch(this);
  }

  /**
   * Sets the number of training epochs (passes) through the data set
   *
   * @param epochs the number of passes through the data set
   */
  public void setEpochs(int epochs) {
    this.epochs = epochs;
  }

  /**
   * Returns the number of passes through the data set
   *
   * @return the number of passes through the data set
   */
  public int getEpochs() {
    return epochs;
  }

  /**
   * Sets the learning rate to use for the algorithm. Unlike many other stochastic algorithms, the learning rate for
   * CSKLR should be large, often in the range of (0.5, 1) - and can even be larger than 1 at times. If the learning
   * rate is too low, it may be difficult to get strong confidence results from the algorithm.
   *
   * @param eta the positive learning rate to use
   */
  public void setEta(double eta) {
    if (eta < 0 || Double.isNaN(eta) || Double.isInfinite(eta)) {
      throw new IllegalArgumentException("The learning rate should be in (0, Inf), not " + eta);
    }
    this.eta = eta;
  }

  /**
   * Returns the learning rate to use
   *
   * @return the learning rate to use
   */
  public double getEta() {
    return eta;
  }

  /**
   * Sets the maximal margin norm value for the algorithm. When the norm is exceeded, the coefficients will be rescaled
   * to fit in the norm. If the maximal norm is too small (less than 5), it may be difficult to get strong confidence
   * results from the algorithm. <br>
   * A good range of values suggested by the original paper is 10<sup>x</sup>
   * &forall; x &isin; {0, 1, 2, 3, 4, 5}
   *
   * @param R
   */
  public void setR(double R) {
    if (R < 0 || Double.isNaN(R) || Double.isInfinite(R)) {
      throw new IllegalArgumentException("The max norm should be in (0, Inf), not " + R);
    }
    this.R = R;
  }

  /**
   * Returns the maximal norm of the algorithm
   *
   * @return the maximal norm of the algorithm
   */
  public double getR() {
    return R;
  }

  /**
   * Sets what update mode should be used. The update mode controls the sparsity of the mode, and the behavior of {@link #setGamma(double)
   * }
   *
   * @param mode the update mode to use
   */
  public void setMode(CSKLR.UpdateMode mode) {
    this.mode = mode;
  }

  /**
   * Returns the update mode in use
   *
   * @return the update mode in use
   */
  public CSKLR.UpdateMode getMode() {
    return mode;
  }

  /**
   * Sets the gamma value to use. This value, depending on which {@link CSKLR.UpdateMode} is used, controls the sparsity
   * of the model.
   *
   * @param gamma the gamma parameter, which is at least always positive
   */
  public void setGamma(double gamma) {
    if (gamma < 0 || Double.isNaN(gamma) || Double.isInfinite(gamma)) {
      throw new IllegalArgumentException("Gamma must be in (0, Infity), not " + gamma);
    }
    this.gamma = gamma;
  }

  /**
   * Returns the gamma sparsity parameter value
   *
   * @return the gamma sparsity parameter value
   */
  public double getGamma() {
    return gamma;
  }

  /**
   * Guesses the distribution to use for the R parameter
   *
   * @param d the dataset to get the guess for
   * @return the guess for the R parameter
   * @see #setR(double)
   */
  public static Distribution guessR(DataSet d) {
    return new LogUniform(1, 1e5);
  }

  @Override
  public List<Parameter> getParameters() {
    return Parameter.getParamsFromMethods(this);
  }

  @Override
  public Parameter getParameter(String paramName) {
    return Parameter.toParameterMap(getParameters()).get(paramName);
  }

  @Override
  public CategoricalResults classify(DataPoint data) {
    CategoricalResults cr = new CategoricalResults(2);

    double p_0 = CSKLR.getScore(-1, getPreScore(data.getNumericalValues()));

    cr.setProb(0, p_0);
    cr.setProb(1, 1 - p_0);

    return cr;
  }

  @Override
  public void trainC(ClassificationDataSet dataSet, ExecutorService threadPool) {
    trainC(dataSet);
  }

  @Override
  public void trainC(ClassificationDataSet dataSet) {
    if (dataSet.getClassSize() != 2) {
      throw new FailedToFitException("CSKLR supports only binary classification");
      //First we need to set up the vectors array
    }

    final int N = dataSet.getSampleSize();
    vecs = new ArrayList<Vec>(N);
    alphas = new double[N];
    for (int i = 0; i < N; i++) {
      vecs.add(dataSet.getDataPoint(i).getNumericalValues());
    }

    curNorm = 0;
    T = 0;
    Random rand = new XORWOW();

    IntList sampleOrder = new IntList(N);
    ListUtils.addRange(sampleOrder, 0, N, 1);

    setCacheMode(getCacheMode());//Initiates the cahce

    for (int epoch = 0; epoch < epochs; epoch++) {
      Collections.shuffle(sampleOrder);

      for (int i : sampleOrder) {
        final double weight = dataSet.getDataPoint(i).getWeight();
        final double y_t = dataSet.getDataPointCategory(i) * 2 - 1;
        final Vec x_t = vecs.get(i);
        final double pre = getPreScore(x_t);
        final double score = CSKLR.getScore(y_t, pre);

        switch (mode) {
          case NC:
            break;
          default:
            double pt = mode.pt(y_t, score, pre, eta, gamma);
            if (rand.nextDouble() > pt) {
              continue;
            }
            break;
        }

        double alpha_i = -eta * y_t * mode.grad(y_t, score, pre, gamma) * weight;

        alphas[i] += alpha_i;

        curNorm += Math.abs(alpha_i) * kEval(i, i);

        //projection step
        if (curNorm > R) {
          double coef = R / curNorm;
          for (int j = 0; j < alphas.length; j++) {
            alphas[j] *= coef;
          }
          curNorm = coef;
        }
      }

    }

    int supportVectorCount = 0;
    for (int i = 0; i < N; i++) {
      if (alphas[i] > 0 || alphas[i] < 0)//Its a support vector
      {
        ListUtils.swap(vecs, supportVectorCount, i);
        alphas[supportVectorCount++] = alphas[i];
      }
    }
    vecs = new ArrayList<Vec>(vecs.subList(0, supportVectorCount));
    alphas = Arrays.copyOfRange(alphas, 0, supportVectorCount);

    setCacheMode(null);
    setAlphas(alphas);
  }

  private double getPreScore(Vec x) {
    return kEvalSum(x);
  }

  @Override
  public boolean supportsWeightedData() {
    return true;
  }

}
