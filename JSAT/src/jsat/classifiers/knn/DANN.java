package jsat.classifiers.knn;

import static java.lang.Math.*;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.concurrent.ExecutorService;
import jsat.DataSet;
import jsat.classifiers.*;
import jsat.distributions.Distribution;
import jsat.distributions.discrete.UniformDiscrete;
import jsat.linear.*;
import jsat.linear.distancemetrics.EuclideanDistance;
import jsat.linear.distancemetrics.MahalanobisDistance;
import jsat.linear.vectorcollection.DefaultVectorCollectionFactory;
import jsat.linear.vectorcollection.VectorCollection;
import jsat.linear.vectorcollection.VectorCollectionFactory;
import jsat.parameters.Parameter;
import jsat.parameters.Parameterized;
import jsat.utils.BoundedSortedList;
import jsat.utils.FakeExecutor;

/**
 * DANN is an implementation of Discriminant Adaptive Nearest Neighbor. DANN has a fixed <i>O(n)</i> classification
 * time. At each classification, DANN uses a large set of points to iteratively create and adjust a distance metic that
 * reflects the separability of classes at a localized level. This increases the work considerably over a normal
 * {@link NearestNeighbour} classifier. The localized metric is similar to the {@link MahalanobisDistance}
 * <br>
 * Because DANN builds its own metric, it is not possible to provide one. The {@link VectorCollectionFactory} allowed in
 * the constructor is to accelerate the first convergence step. In homogeneous areas of the data set, queries can be
 * answered in <i>O(log n)</i> if the vector collection supports it.
 * <br><br>
 * See: Hastie, T.,&amp;Tibshirani, R. (1996). <i>Discriminant adaptive nearest neighbor classification</i>. IEEE
 * Transactions on Pattern Analysis and Machine Intelligence, 18(6), 607â€“616. doi:10.1109/34.506411
 *
 * @author Edward Raff
 */
public class DANN implements Classifier, Parameterized {

  private static final long serialVersionUID = -272865942127664672L;
  /**
   * The default number of neighbors to use when building a metric is {@value #DEFAULT_KN}.
   */
  public static final int DEFAULT_KN = 40;
  /**
   * The default number of neighbors to use when classifying is {@value #DEFAULT_K}
   */
  public static final int DEFAULT_K = 1;
  /**
   * The default regularization used when building a metric is {@value #DEFAULT_EPS}
   */
  public static final double DEFAULT_EPS = 1.0;
  /**
   * The default number of iterations for creating the metric is {@value #DEFAULT_ITERATIONS}
   */
  public static final int DEFAULT_ITERATIONS = 1;

  private int kn;
  private int k;
  private int maxIterations;
  private double eps;

  private VectorCollectionFactory<VecPaired<Vec, Integer>> vcf;

  private CategoricalData predicting;

  /**
   * Vectors paired with their index in the original data set
   */
  private VectorCollection<VecPaired<Vec, Integer>> vc;
  private List<VecPaired<Vec, Integer>> vecList;

  /**
   * Creates a new DANN classifier
   */
  public DANN() {
    this(DEFAULT_KN, DEFAULT_K);
  }

  /**
   * Creates a new DANN classifier
   *
   * @param kn the number of neighbors to use in casting a net to build a new metric
   * @param k the number of neighbors to use with the final metric in classification
   */
  public DANN(int kn, int k) {
    this(kn, k, DEFAULT_EPS);
  }

  /**
   * Creates a new DANN classifier
   *
   * @param kn the number of neighbors to use in casting a net to build a new metric
   * @param k the number of neighbors to use with the final metric in classification
   * @param eps the regularization to add to the metric creation
   */
  public DANN(int kn, int k, double eps) {
    this(kn, k, eps, new DefaultVectorCollectionFactory<VecPaired<Vec, Integer>>());
  }

  /**
   * Creates a new DANN classifier
   *
   * @param kn the number of neighbors to use in casting a net to build a new metric
   * @param k the number of neighbors to use with the final metric in classification
   * @param eps the regularization to add to the metric creation
   * @param vcf the default vector collection that will be used for initial neighbor search
   */
  public DANN(int kn, int k, double eps, VectorCollectionFactory<VecPaired<Vec, Integer>> vcf) {
    this(kn, k, eps, DEFAULT_ITERATIONS, vcf);
  }

  /**
   * Creates a new DANN classifier
   *
   * @param kn the number of neighbors to use in casting a net to build a new metric
   * @param k the number of neighbors to use with the final metric in classification
   * @param eps the regularization to add to the metric creation
   * @param maxIterations the maximum number of times to adjust the metric for each classification
   * @param vcf the default vector collection that will be used for initial neighbor search
   */
  public DANN(int kn, int k, double eps, int maxIterations, VectorCollectionFactory<VecPaired<Vec, Integer>> vcf) {
    setK(k);
    setKn(kn);
    setEpsilon(eps);
    setMaxIterations(maxIterations);
    this.vcf = vcf;
  }

  /**
   * Sets the number of nearest neighbors to use when predicting
   *
   * @param k the number of neighbors
   */
  public void setK(int k) {
    if (k < 1) {
      throw new ArithmeticException("Number of neighbors must be positive");
    }
    this.k = k;
  }

  /**
   * Returns the number of nearest neighbors to use when predicting
   *
   * @return the number of neighbors used for classification
   */
  public int getK() {
    return k;
  }

  /**
   * Sets the number of nearest neighbors to use when adapting the distance metric. At each iteration of the algorithm,
   * a new distance metric will be created. A larger number of neighbors is used to create a net of points, around which
   * the metric will be adapted.
   *
   * @param kn the number of neighbors to use
   */
  public void setKn(int kn) {
    if (kn < 2) {
      throw new ArithmeticException("At least 2 neighbors are needed to adapat the metric");
    }
    this.kn = kn;
  }

  /**
   * Returns the number of nearest neighbors to use when adapting the distance metric
   *
   * @return the number of neighbors used to adapt the metric
   */
  public int getKn() {
    return kn;
  }

  /**
   * Sets the number of times a new distance metric will be created for each query. The metric should converge quickly.
   * For this reason, and do to a lack of performance difference, it is highly recommended to use the default of 1
   * iteration.
   *
   * @param maxIterations the maximum number of times the metric will be updated
   */
  public void setMaxIterations(int maxIterations) {
    if (maxIterations < 1) {
      throw new RuntimeException("At least one iteration must be performed");
    }
    this.maxIterations = maxIterations;
  }

  /**
   * Returns the number of times the distance metric will be updated.
   *
   * @return the number of iterations the metric will be updated
   */
  public int getMaxIterations() {
    return maxIterations;
  }

  /**
   * Sets the regularization to apply the the diagonal of the scatter matrix when creating each new metric.
   *
   * @param eps the regularization value
   */
  public void setEpsilon(double eps) {
    if (eps < 0 || Double.isInfinite(eps) || Double.isNaN(eps)) {
      throw new ArithmeticException("Regularization must be a positive value");
    }
    this.eps = eps;
  }

  /**
   * Returns the regularization parameter that is applied to the diagonal of the matrix when creating each new metric.
   *
   * @return the regularization used.
   */
  public double getEpsilon() {
    return eps;
  }

  @Override
  public CategoricalResults classify(DataPoint data) {
    CategoricalResults cr = new CategoricalResults(predicting.getNumOfCategories());

    int n = data.numNumericalValues();
    Matrix sigma = Matrix.eye(n);
    Matrix B = new DenseMatrix(n, n);
    Matrix W = new DenseMatrix(n, n);
    Vec query = data.getNumericalValues();

    Vec scratch0 = new DenseVector(n);

    //TODO threadlocal DoubleList / DenseVec might be better in practice for memory use
    double[] weights = new double[kn];
    double[] priors = new double[predicting.getNumOfCategories()];
    int[] classCount = new int[priors.length];
    double sumOfWeights;
    Vec mean = new DenseVector(sigma.rows());
    Vec[] classMeans = new Vec[predicting.getNumOfCategories()];
    for (int i = 0; i < classMeans.length; i++) {
      classMeans[i] = new DenseVector(mean.length());
    }

    for (int iter = 0; iter < maxIterations; iter++) {
      //Zero out prev iter
      mean.zeroOut();
      Arrays.fill(priors, 0);
      Arrays.fill(classCount, 0);
      for (Vec classMean : classMeans) {
        classMean.zeroOut();
      }
      sumOfWeights = 0;
      B.zeroOut();
      W.zeroOut();

      List<? extends VecPaired<VecPaired<Vec, Integer>, Double>> vecs
              = (iter == 0) ? vc.search(query, kn) : brute(query, sigma, kn);
      //Compute vector mean and weight sums, class weight sums, and the class means
      double h = vecs.get(vecs.size() - 1).getPair();
      for (int i = 0; i < vecs.size(); i++) {
        VecPaired<? extends VecPaired<Vec, Integer>, Double> vec = vecs.get(i);
        //vecs contains the distances, we need the distance squared
        weights[i] = pow(pow(1 - pow(vec.getPair(), 2) / h, 3), 3);
        sumOfWeights += weights[i];
        mean.mutableAdd(vec);

        int j = vec.getVector().getPair();
        priors[j] += weights[i];
        classMeans[j].mutableAdd(vec);
        classCount[j]++;
      }

      //Final divide for means and priors
      mean.mutableDivide(kn);
      for (int i = 0; i < classMeans.length; i++) {
        if (classCount[i] != 0.0) {
          classMeans[i].mutableDivide(classCount[i]);
        }
        priors[i] /= sumOfWeights;
      }

      //Compute B & W
      for (int j = 0; j < classMeans.length; j++) {
        //One line for B's work
        if (priors[j] > 0) {
          classMeans[j].copyTo(scratch0);
          scratch0.mutableSubtract(mean);

          Matrix.OuterProductUpdate(B, scratch0, scratch0, priors[j]);

          //Loop for W's work
          for (int i = 0; i < vecs.size(); i++) {
            VecPaired<? extends VecPaired<Vec, Integer>, Double> x = vecs.get(i);
            if (x.getVector().getPair() == j) {
              x.copyTo(scratch0);
              scratch0.mutableSubtract(classMeans[j]);

              Matrix.OuterProductUpdate(W, scratch0, scratch0, weights[i]);
            }
          }
        }
      }

      //Final divide for W
      W.mutableMultiply(1.0 / sumOfWeights);

      RowColumnOps.addDiag(B, 0, B.rows(), eps);

      //Check, if there is a prior of 1, nothing will ever be updated. 
      //Might as well return
      for (int i = 0; i < priors.length; i++) {
        if (priors[i] == 1.0) {
          cr.setProb(i, 1.0);
          return cr;
        }
      }

      EigenValueDecomposition evd = new EigenValueDecomposition(W);
      Matrix D = evd.getD();
      for (int i = 0; i < D.rows(); i++) {
        D.set(i, i, pow(D.get(i, i), -0.5));
      }
      Matrix VT = evd.getVT();
      Matrix WW = VT.transposeMultiply(D).multiply(VT);

      sigma.zeroOut();
      WW.multiply(B).multiply(WW, sigma);
    }

    List<? extends VecPaired<? extends VecPaired<Vec, Integer>, Double>> knn
            = brute(query, sigma, k);

    for (VecPaired<? extends VecPaired<Vec, Integer>, Double> nn : knn) {
      cr.incProb(nn.getVector().getPair(), 1.0);
    }

    cr.normalize();
    return cr;
  }

  @Override
  public void trainC(ClassificationDataSet dataSet, ExecutorService threadPool) {
    predicting = dataSet.getPredicting();
    vecList = new ArrayList<VecPaired<Vec, Integer>>(dataSet.getSampleSize());
    for (int i = 0; i < dataSet.getSampleSize(); i++) {
      vecList.add(new VecPaired<Vec, Integer>(dataSet.getDataPoint(i).getNumericalValues(), dataSet.getDataPointCategory(i)));
    }
    if (threadPool == null || threadPool instanceof FakeExecutor) {
      vc = vcf.getVectorCollection(vecList, new EuclideanDistance());
    } else {
      vc = vcf.getVectorCollection(vecList, new EuclideanDistance(), threadPool);
    }
  }

  @Override
  public void trainC(ClassificationDataSet dataSet) {
    trainC(dataSet, null);
  }

  @Override
  public boolean supportsWeightedData() {
    return false;
  }

  @Override
  public Classifier clone() {
    DANN clone = new DANN(kn, k, maxIterations, vcf.clone());

    if (this.predicting != null) {
      clone.predicting = this.predicting.clone();
    }
    if (this.vc != null) {
      clone.vc = this.vc.clone();
    }
    if (this.vecList != null) {
      clone.vecList = new ArrayList<VecPaired<Vec, Integer>>(this.vecList);
    }

    return clone;
  }

  private double dist(Matrix sigma, Vec query, Vec mean, Vec scratch0, Vec scartch1) {
    query.copyTo(scratch0);
    scratch0.mutableSubtract(mean);

    scartch1.zeroOut();
    sigma.multiply(scratch0, 1.0, scartch1);

    return scratch0.dot(scartch1);
  }

  private List<? extends VecPaired<VecPaired<Vec, Integer>, Double>> brute(Vec query, Matrix sigma, int num) {
    Vec scartch0 = new DenseVector(query.length());
    Vec scartch1 = new DenseVector(query.length());
    BoundedSortedList<VecPairedComparable<VecPaired<Vec, Integer>, Double>> knn = new BoundedSortedList<VecPairedComparable<VecPaired<Vec, Integer>, Double>>(num, num);

    for (VecPaired<Vec, Integer> v : vecList) {
      double d = dist(sigma, query, v, scartch0, scartch1);
      knn.add(new VecPairedComparable<VecPaired<Vec, Integer>, Double>(v, d));
    }

    return (List<VecPaired<VecPaired<Vec, Integer>, Double>>) (List<? extends VecPaired<VecPaired<Vec, Integer>, Double>>) knn;
  }

  /**
   * Guesses the distribution to use for the number of neighbors to consider
   *
   * @param d the dataset to get the guess for
   * @return the guess for the K parameter
   */
  public static Distribution guessK(DataSet d) {
    return new UniformDiscrete(1, 25);
  }

  /**
   * Guesses the distribution to use for the number of neighbors to consider
   *
   * @param d the dataset to get the guess for
   * @return the guess for the Kn parameter
   */
  public static Distribution guessKn(DataSet d) {
    return new UniformDiscrete(40, Math.max(d.getSampleSize() / 5, 50));
  }

  @Override
  public List<Parameter> getParameters() {
    return Parameter.getParamsFromMethods(this);
  }

  @Override
  public Parameter getParameter(String paramName) {
    return Parameter.toParameterMap(getParameters()).get(paramName);
  }
}
